# Intermediate ChatGPT
---
### Advancements of transformer architecture
* The transformer architecture, introduced by Vaswani et al. in 2017, brought significant advancements over the previously used Recurrent Neural Networks (RNNs).
* What key advancement does the transformer architecture provide over RNNs?

Attention mechanisms that weigh the significance of each word, irrespective of its position.
### Features of RNNs versus transformers
* When comparing different types of neural network architectures, it’s important to understand the distinctive features and advantages of each. Recurrent Neural Networks (RNNs) and Transformer models are two such architectures, each with their own unique characteristics and applications.

### Features of RNNs versus transformers
* When comparing different types of neural network architectures, it’s important to understand the distinctive features and advantages of each. Recurrent Neural Networks (RNNs) and Transformer models are two such architectures, each with their own unique characteristics and applications.
---
### The role of decoders
* In the Transformer architecture, encoding and decoding are two fundamental processes. Understanding these processes is crucial for tasks like language translation and text generation.
> To convert a sequence of continuous representations back into an output sequence.

### Encoding versus Decoding
*Encoding*
- Mapping an input sequence of tokens to a sequence of continuous representations
- Assigning unique identifiers called token IDs
   
*Decoding*
